{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aed98a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden size: 300 latent_size: 100 depth: 2\n",
      "loading data.....\n",
      "size of reactant dic: 9766\n",
      "size of template dic: 5567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21218/21218 [02:42<00:00, 130.80it/s]\n",
      "/home/gzou/brxngenerator/.venv/lib/python3.12/site-packages/torch/nn/_reduction.py:51: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of fgm_trees: 20080\n",
      "size of rxn_trees: 20080\n",
      "size of fragment dic: 273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gzou/brxngenerator/.venv/lib/python3.12/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished loading model...\n",
      "number of samples: 20080\n",
      "========start to compute all scores\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20080it [06:35, 50.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================latent shape (20080, 50)\n",
      "(18072, 50) (2008, 50) (18072, 1) (2008, 1)\n",
      "Using Gurobi with provided options: {'LICENSEID': 2687913, 'WLSACCESSID': '5cbfb8e1-0066-4b7f-ab40-579464946573', 'WLSSECRET': 'a5c475ea-ec91-4cd6-94e9-b73395e273d6'}\n",
      "Set parameter WLSAccessID\n",
      "Set parameter WLSSecret\n",
      "Set parameter LicenseID to value 2687913\n",
      "Academic license 2687913 - for non-commercial use only - registered to 89___@edu.k.u-tokyo.ac.jp\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# python import\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import logging\n",
    "import random\n",
    "import sys\n",
    "sys.path.append('./rxnft_vae')\n",
    "\n",
    "# rxnft_vae imports\n",
    "from rxnft_vae.reaction import ReactionTree, extract_starting_reactants, StartingReactants, Templates, extract_templates\n",
    "from rxnft_vae.fragment import FragmentVocab, FragmentTree\n",
    "from rxnft_vae.vae import bFTRXNVAE\n",
    "from rxnft_vae.mpn import MPN\n",
    "from rxnft_vae.reaction_utils import read_multistep_rxns, get_qed_score,get_clogp_score\n",
    "\n",
    "# torch\n",
    "import torch\n",
    "\n",
    "# tqdm\n",
    "from tqdm import tqdm\n",
    "\n",
    "# my binary vae utils\n",
    "import binary_vae_utils\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def seed_all(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n",
    "\n",
    "hidden_size = 300\n",
    "\n",
    "latent_size = 100\n",
    "\n",
    "depth = 2\n",
    "\n",
    "data_filename = \"./data/data.txt\"\n",
    "w_save_path = \"./weights/hidden_size_300_latent_size_100_depth_2_beta_1.0_lr_0.001/bvae_iter-30-with.npy\"\n",
    "metric = \"qed\"\n",
    "\n",
    "seed = binary_vae_utils.RANDOM_SEED\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"hidden size:\", hidden_size, \"latent_size:\", latent_size, \"depth:\", depth)\n",
    "print(\"loading data.....\")\n",
    "routes, scores = read_multistep_rxns(data_filename)\n",
    "rxn_trees = [ReactionTree(route) for route in routes]\n",
    "molecules = [rxn_tree.molecule_nodes[0].smiles for rxn_tree in rxn_trees]\n",
    "reactants = extract_starting_reactants(rxn_trees)\n",
    "templates, n_reacts = extract_templates(rxn_trees)\n",
    "reactantDic = StartingReactants(reactants)\n",
    "templateDic = Templates(templates, n_reacts)\n",
    "\n",
    "print(\"size of reactant dic:\", reactantDic.size())\n",
    "print(\"size of template dic:\", templateDic.size())\n",
    "\n",
    "n_pairs = len(routes)\n",
    "ind_list = [i for i in range(n_pairs)]\n",
    "\n",
    "fgm_trees = []\n",
    "valid_id = []\n",
    "for i in tqdm(ind_list):\n",
    "    try:\n",
    "        fgm_trees.append(FragmentTree(rxn_trees[i].molecule_nodes[0].smiles))\n",
    "        valid_id.append(i)\n",
    "    except Exception as e:\n",
    "        # print(e)\n",
    "        continue\n",
    "rxn_trees = [rxn_trees[i] for i in valid_id]\n",
    "\n",
    "print(\"size of fgm_trees:\", len(fgm_trees))\n",
    "print(\"size of rxn_trees:\", len(rxn_trees))\n",
    "data_pairs = []\n",
    "for fgm_tree, rxn_tree in zip(fgm_trees, rxn_trees):\n",
    "    data_pairs.append((fgm_tree, rxn_tree))\n",
    "cset = set()\n",
    "for fgm_tree in fgm_trees:\n",
    "    for node in fgm_tree.nodes:\n",
    "        cset.add(node.smiles)\n",
    "cset = list(cset)\n",
    "fragmentDic = FragmentVocab(cset)\n",
    "\n",
    "print(\"size of fragment dic:\", fragmentDic.size())\n",
    "\n",
    "\n",
    "mpn = MPN(hidden_size, depth)\n",
    "model = bFTRXNVAE(fragmentDic, reactantDic, templateDic, hidden_size, latent_size, depth, device,\n",
    "                    fragment_embedding=None, reactant_embedding=None, template_embedding=None).to(device)\n",
    "checkpoint = torch.load(w_save_path, map_location=device)\n",
    "model.load_state_dict(checkpoint)\n",
    "print(\"finished loading model...\")\n",
    "\n",
    "\n",
    "seed_all(seed)\n",
    "\n",
    "\n",
    "X_train, y_train, X_test, y_test = binary_vae_utils.prepare_dataset(model=model, data_pairs=data_pairs,latent_size=latent_size)\n",
    "\n",
    "X_train = torch.Tensor(X_train)\n",
    "y_train = torch.Tensor(y_train)\n",
    "X_test = torch.Tensor(X_test)\n",
    "y_test = torch.Tensor(y_test)\n",
    "\n",
    "FM_surrogate = binary_vae_utils.FactorizationMachineSurrogate(n_binary=latent_size//2,k_factors=binary_vae_utils.FACTOR_NUM,random_seed=seed)\n",
    "\n",
    "options = {\n",
    "    \"LICENSEID\": 2687913,\n",
    "    \"WLSACCESSID\": \"5cbfb8e1-0066-4b7f-ab40-579464946573\",\n",
    "    \"WLSSECRET\": \"a5c475ea-ec91-4cd6-94e9-b73395e273d6\"\n",
    "}\n",
    "\n",
    "gurobi_solver = binary_vae_utils.GurobiQuboSolver(options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08b23568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Iteration 0 ---\n",
      "========shape:  torch.Size([18072, 50]) torch.Size([18072, 1]) torch.Size([2008, 50]) torch.Size([2008, 1])\n",
      "lr:  0.001\n",
      "Model -- Epoch 0 error on validation set: 0.1177, r2 on validation set: -1.6264\n",
      "Model -- Epoch 100 error on validation set: 0.0438, r2 on validation set: 0.0235\n",
      "Model -- Epoch 200 error on validation set: 0.0437, r2 on validation set: 0.0249\n",
      "Model -- Epoch 300 error on validation set: 0.0435, r2 on validation set: 0.0295\n",
      "Model -- Epoch 400 error on validation set: 0.0436, r2 on validation set: 0.0273\n",
      "Model -- Epoch 500 error on validation set: 0.0439, r2 on validation set: 0.0205\n",
      "Model -- Epoch 600 error on validation set: 0.0438, r2 on validation set: 0.0227\n",
      "Model -- Epoch 700 error on validation set: 0.0436, r2 on validation set: 0.0281\n",
      "Model -- Epoch 800 error on validation set: 0.0434, r2 on validation set: 0.0310\n",
      "Model -- Epoch 900 error on validation set: 0.0433, r2 on validation set: 0.0337\n",
      "Model -- Epoch 1000 error on validation set: 0.0440, r2 on validation set: 0.0192\n",
      "Model -- Epoch 734 has lowest error!\n",
      "(2008, 1, 1) (2008, 1)\n",
      "best torchfm model loaded.\n",
      "========Updating dataset with new solutions=========\n",
      "[[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1.\n",
      "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "  1. 0.]]\n",
      "========binary_new shape\n",
      "torch.Size([1, 50])\n",
      "shape of binary_new torch.Size([1, 50])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5000 [00:00<?, ?it/s]/home/gzou/brxngenerator/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n",
      "  1%|          | 40/5000 [00:10<22:06,  3.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========res\n",
      "[['Cc1cc2c(NCCc3ccc4c(c3)OCO4)nc(-n3ccnc3)nc2s1', 'Cc1cc2c(NCCc3ccc4c(c3)OCO4)nc(-n3ccnc3)nc2s1*Cc1cc2c(NCCc3ccc4c(c3)OCO4)nc(Cl)nc2s1.c1c[nH]cn1*([#16;a:5]:[c:4]:[#7;a:3]:[c;H0;D3;+0:1](:[#7;a:2])-[n;H0;D3;+0:8]1:[c:7]:[#7;a:6]:[c:10]:[c:9]:1)>>Cl-[c;H0;D3;+0:1](:[#7;a:2]):[#7;a:3]:[c:4]:[#16;a:5].[#7;a:6]1:[c:7]:[nH;D2;+0:8]:[c:9]:[c:10]:1 Cc1cc2c(NCCc3ccc4c(c3)OCO4)nc(Cl)nc2s1*NCCc1ccc2c(c1)OCO2.Cc1cc2c(Cl)nc(Cl)nc2s1*([#16;a:5]:[c:4](:[#7;a:6]):[c:3]:[c;H0;D3;+0:1](:[#7;a:2])-[NH;D2;+0:8]-[C:7])>>Cl-[c;H0;D3;+0:1](:[#7;a:2]):[c:3]:[c:4](:[#16;a:5]):[#7;a:6].[C:7]-[NH2;D1;+0:8]'], ['COc1ccc(CNc2nc(-n3ccnc3)nc3sc(C)c(Cl)c23)cc1OC', 'COc1ccc(CNc2nc(-n3ccnc3)nc3sc(C)c(Cl)c23)cc1OC*c1c[nH]cn1.COc1ccc(CNc2nc(Cl)nc3sc(C)c(Cl)c23)cc1OC*([#16;a:5]:[c:4]:[#7;a:3]:[c;H0;D3;+0:1](:[#7;a:2])-[n;H0;D3;+0:8]1:[c:7]:[#7;a:6]:[c:10]:[c:9]:1)>>Cl-[c;H0;D3;+0:1](:[#7;a:2]):[#7;a:3]:[c:4]:[#16;a:5].[#7;a:6]1:[c:7]:[nH;D2;+0:8]:[c:9]:[c:10]:1 COc1ccc(CNc2nc(Cl)nc3sc(C)c(Cl)c23)cc1OC*COc1ccc(CN)cc1OC.Cc1sc2nc(Cl)nc(Cl)c2c1Cl*([#16;a:5]:[c:4](:[#7;a:6]):[c:3]:[c;H0;D3;+0:1](:[#7;a:2])-[NH;D2;+0:8]-[C:7])>>Cl-[c;H0;D3;+0:1](:[#7;a:2]):[c:3]:[c:4](:[#16;a:5]):[#7;a:6].[C:7]-[NH2;D1;+0:8] Cc1sc2nc(Cl)nc(Cl)c2c1Cl*Cc1cc2c(Cl)nc(Cl)nc2s1.O=C1CCC(=O)N1Cl*([#7;a:2]:[c:3]1:[#16;a:4]:[c:5]:[c;H0;D3;+0:6](-[Cl;H0;D1;+0:1]):[c:7]:1:[c:8]:[#7;a:9])>>O=C1-C-C-C(=O)-N-1-[Cl;H0;D1;+0:1].[#7;a:2]:[c:3]1:[#16;a:4]:[c:5]:[cH;D2;+0:6]:[c:7]:1:[c:8]:[#7;a:9]'], ['COc1ccc(CNc2nc(NCCCO)nc3sc(C)c(Cl)c23)cc1Cl', 'COc1ccc(CNc2nc(NCCCO)nc3sc(C)c(Cl)c23)cc1Cl*NCCCO.COc1ccc(CNc2nc(Cl)nc3sc(C)c(Cl)c23)cc1Cl*([#7;a:2]:[c;H0;D3;+0:1](:[#7;a:3])-[NH;D2;+0:5]-[C:4])>>Cl-[c;H0;D3;+0:1](:[#7;a:2]):[#7;a:3].[C:4]-[NH2;D1;+0:5] COc1ccc(CNc2nc(Cl)nc3sc(C)c(Cl)c23)cc1Cl*Cc1sc2nc(Cl)nc(Cl)c2c1Cl.COc1ccc(CN)cc1Cl*([#16;a:5]:[c:4](:[#7;a:6]):[c:3]:[c;H0;D3;+0:1](:[#7;a:2])-[NH;D2;+0:8]-[C:7])>>Cl-[c;H0;D3;+0:1](:[#7;a:2]):[c:3]:[c:4](:[#16;a:5]):[#7;a:6].[C:7]-[NH2;D1;+0:8] Cc1sc2nc(Cl)nc(Cl)c2c1Cl*Cc1cc2c(Cl)nc(Cl)nc2s1.O=C1CCC(=O)N1Cl*([#7;a:2]:[c:3]1:[#16;a:4]:[c:5]:[c;H0;D3;+0:6](-[Cl;H0;D1;+0:1]):[c:7]:1:[c:8]:[#7;a:9])>>O=C1-C-C-C(=O)-N-1-[Cl;H0;D1;+0:1].[#7;a:2]:[c:3]1:[#16;a:4]:[c:5]:[cH;D2;+0:6]:[c:7]:1:[c:8]:[#7;a:9]'], ['COc1ccc(CNc2nc(-n3ccnc3)nc3sc(C)cc23)cc1OC', 'COc1ccc(CNc2nc(-n3ccnc3)nc3sc(C)cc23)cc1OC*c1c[nH]cn1.COc1ccc(CNc2nc(Cl)nc3sc(C)cc23)cc1OC*([#16;a:5]:[c:4]:[#7;a:3]:[c;H0;D3;+0:1](:[#7;a:2])-[n;H0;D3;+0:8]1:[c:7]:[#7;a:6]:[c:10]:[c:9]:1)>>Cl-[c;H0;D3;+0:1](:[#7;a:2]):[#7;a:3]:[c:4]:[#16;a:5].[#7;a:6]1:[c:7]:[nH;D2;+0:8]:[c:9]:[c:10]:1 COc1ccc(CNc2nc(Cl)nc3sc(C)cc23)cc1OC*COc1ccc(CN)cc1OC.Cc1cc2c(Cl)nc(Cl)nc2s1*([#16;a:5]:[c:4](:[#7;a:6]):[c:3]:[c;H0;D3;+0:1](:[#7;a:2])-[NH;D2;+0:8]-[C:7])>>Cl-[c;H0;D3;+0:1](:[#7;a:2]):[c:3]:[c:4](:[#16;a:5]):[#7;a:6].[C:7]-[NH2;D1;+0:8]'], ['COc1ccc(CNc2nc(-n3ccnc3)nc3sc(C)cc23)cc1OC', 'COc1ccc(CNc2nc(-n3ccnc3)nc3sc(C)cc23)cc1OC*c1c[nH]cn1.COc1ccc(CNc2nc(Cl)nc3sc(C)cc23)cc1OC*([#16;a:5]:[c:4]:[#7;a:3]:[c;H0;D3;+0:1](:[#7;a:2])-[n;H0;D3;+0:8]1:[c:7]:[#7;a:6]:[c:10]:[c:9]:1)>>Cl-[c;H0;D3;+0:1](:[#7;a:2]):[#7;a:3]:[c:4]:[#16;a:5].[#7;a:6]1:[c:7]:[nH;D2;+0:8]:[c:9]:[c:10]:1 COc1ccc(CNc2nc(Cl)nc3sc(C)cc23)cc1OC*COc1ccc(CN)cc1OC.Cc1cc2c(Cl)nc(Cl)nc2s1*([#16;a:5]:[c:4](:[#7;a:6]):[c:3]:[c;H0;D3;+0:1](:[#7;a:2])-[NH;D2;+0:8]-[C:7])>>Cl-[c;H0;D3;+0:1](:[#7;a:2]):[c:3]:[c:4](:[#16;a:5]):[#7;a:6].[C:7]-[NH2;D1;+0:8]'], ['COc1ccc(CNc2nc(-n3ccnc3)nc3sc(C)cc23)cc1OC', 'COc1ccc(CNc2nc(-n3ccnc3)nc3sc(C)cc23)cc1OC*c1c[nH]cn1.COc1ccc(CNc2nc(Cl)nc3sc(C)cc23)cc1OC*([#16;a:5]:[c:4]:[#7;a:3]:[c;H0;D3;+0:1](:[#7;a:2])-[n;H0;D3;+0:8]1:[c:7]:[#7;a:6]:[c:10]:[c:9]:1)>>Cl-[c;H0;D3;+0:1](:[#7;a:2]):[#7;a:3]:[c:4]:[#16;a:5].[#7;a:6]1:[c:7]:[nH;D2;+0:8]:[c:9]:[c:10]:1 COc1ccc(CNc2nc(Cl)nc3sc(C)cc23)cc1OC*COc1ccc(CN)cc1OC.Cc1cc2c(Cl)nc(Cl)nc2s1*([#16;a:5]:[c:4](:[#7;a:6]):[c:3]:[c;H0;D3;+0:1](:[#7;a:2])-[NH;D2;+0:8]-[C:7])>>Cl-[c;H0;D3;+0:1](:[#7;a:2]):[c:3]:[c:4](:[#16;a:5]):[#7;a:6].[C:7]-[NH2;D1;+0:8]']]\n",
      "Number of new molecules: 4\n",
      "========computing qed of molecule0\n",
      "========computing qed of molecule1\n",
      "========computing qed of molecule2\n",
      "========computing qed of molecule3\n",
      "========Updating training set\n",
      "X_train shape before update: torch.Size([18072, 50])\n",
      "y_train shape before update: torch.Size([18072, 1])\n",
      "X_train shape after update: torch.Size([18073, 50])\n",
      "y_train shape after update: torch.Size([18073, 1])\n",
      "Writing to file: ./Results/42_qed.txt\n",
      "--- Starting Iteration 1 ---\n",
      "========shape:  torch.Size([18073, 50]) torch.Size([18073, 1]) torch.Size([2008, 50]) torch.Size([2008, 1])\n",
      "lr:  0.001\n",
      "Model -- Epoch 0 error on validation set: 0.1739, r2 on validation set: -2.8804\n",
      "Model -- Epoch 100 error on validation set: 0.0437, r2 on validation set: 0.0249\n",
      "Model -- Epoch 200 error on validation set: 0.0437, r2 on validation set: 0.0248\n",
      "Model -- Epoch 300 error on validation set: 0.0435, r2 on validation set: 0.0291\n",
      "Model -- Epoch 400 error on validation set: 0.0436, r2 on validation set: 0.0270\n",
      "Model -- Epoch 500 error on validation set: 0.0436, r2 on validation set: 0.0279\n",
      "Model -- Epoch 600 error on validation set: 0.0435, r2 on validation set: 0.0285\n",
      "Model -- Epoch 700 error on validation set: 0.0434, r2 on validation set: 0.0305\n",
      "Model -- Epoch 800 error on validation set: 0.0433, r2 on validation set: 0.0333\n",
      "Model -- Epoch 900 error on validation set: 0.0435, r2 on validation set: 0.0304\n",
      "Model -- Epoch 1000 error on validation set: 0.0442, r2 on validation set: 0.0146\n",
      "Model -- Epoch 775 has lowest error!\n",
      "(2008, 1, 1) (2008, 1)\n",
      "best torchfm model loaded.\n",
      "========Updating dataset with new solutions=========\n",
      "[[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1.\n",
      "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "  1. 0.]]\n",
      "========binary_new shape\n",
      "torch.Size([1, 50])\n",
      "shape of binary_new torch.Size([1, 50])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5000 [00:00<?, ?it/s]/home/gzou/brxngenerator/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n",
      "  0%|          | 16/5000 [00:04<22:40,  3.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========res\n",
      "[['COc1ccc(CNc2nc(-n3ccnc3)nc3sc(C)cc23)cc1OC', 'COc1ccc(CNc2nc(-n3ccnc3)nc3sc(C)cc23)cc1OC*c1c[nH]cn1.COc1ccc(CNc2nc(Cl)nc3sc(C)cc23)cc1OC*([#16;a:5]:[c:4]:[#7;a:3]:[c;H0;D3;+0:1](:[#7;a:2])-[n;H0;D3;+0:8]1:[c:7]:[#7;a:6]:[c:10]:[c:9]:1)>>Cl-[c;H0;D3;+0:1](:[#7;a:2]):[#7;a:3]:[c:4]:[#16;a:5].[#7;a:6]1:[c:7]:[nH;D2;+0:8]:[c:9]:[c:10]:1 COc1ccc(CNc2nc(Cl)nc3sc(C)cc23)cc1OC*COc1ccc(CN)cc1OC.Cc1cc2c(Cl)nc(Cl)nc2s1*([#16;a:5]:[c:4](:[#7;a:6]):[c:3]:[c;H0;D3;+0:1](:[#7;a:2])-[NH;D2;+0:8]-[C:7])>>Cl-[c;H0;D3;+0:1](:[#7;a:2]):[c:3]:[c:4](:[#16;a:5]):[#7;a:6].[C:7]-[NH2;D1;+0:8]'], ['CCc1cc2c(NCc3ccc(OC)c(OC)c3)nc(-n3ccnc3)nc2s1', 'CCc1cc2c(NCc3ccc(OC)c(OC)c3)nc(-n3ccnc3)nc2s1*c1c[nH]cn1.CCc1cc2c(NCc3ccc(OC)c(OC)c3)nc(Cl)nc2s1*([#16;a:5]:[c:4]:[#7;a:3]:[c;H0;D3;+0:1](:[#7;a:2])-[n;H0;D3;+0:8]1:[c:7]:[#7;a:6]:[c:10]:[c:9]:1)>>Cl-[c;H0;D3;+0:1](:[#7;a:2]):[#7;a:3]:[c:4]:[#16;a:5].[#7;a:6]1:[c:7]:[nH;D2;+0:8]:[c:9]:[c:10]:1 CCc1cc2c(NCc3ccc(OC)c(OC)c3)nc(Cl)nc2s1*COc1ccc(CN)cc1OC.CCc1cc2c(Cl)nc(Cl)nc2s1*([#16;a:5]:[c:4](:[#7;a:6]):[c:3]:[c;H0;D3;+0:1](:[#7;a:2])-[NH;D2;+0:8]-[C:7])>>Cl-[c;H0;D3;+0:1](:[#7;a:2]):[c:3]:[c:4](:[#16;a:5]):[#7;a:6].[C:7]-[NH2;D1;+0:8]'], ['COc1ccc(CNc2nc(-n3ccnc3C)nc3sc(Cl)cc23)cc1OC', 'COc1ccc(CNc2nc(-n3ccnc3C)nc3sc(Cl)cc23)cc1OC*Cc1ncc[nH]1.COc1ccc(CNc2nc(Cl)nc3sc(Cl)cc23)cc1OC*([#16;a:5]:[c:4]:[#7;a:3]:[c;H0;D3;+0:1](:[#7;a:2])-[n;H0;D3;+0:8]1:[c:7]:[#7;a:6]:[c:10]:[c:9]:1)>>Cl-[c;H0;D3;+0:1](:[#7;a:2]):[#7;a:3]:[c:4]:[#16;a:5].[#7;a:6]1:[c:7]:[nH;D2;+0:8]:[c:9]:[c:10]:1 COc1ccc(CNc2nc(Cl)nc3sc(Cl)cc23)cc1OC*COc1ccc(CN)cc1OC.Clc1nc(Cl)c2cc(Cl)sc2n1*([#16;a:5]:[c:4](:[#7;a:6]):[c:3]:[c;H0;D3;+0:1](:[#7;a:2])-[NH;D2;+0:8]-[C:7])>>Cl-[c;H0;D3;+0:1](:[#7;a:2]):[c:3]:[c:4](:[#16;a:5]):[#7;a:6].[C:7]-[NH2;D1;+0:8] Clc1nc(Cl)c2cc(Cl)sc2n1*O=C1CCC(=O)N1Cl.Clc1nc(Cl)c2ccsc2n1*([#16;a:2]:[c;H0;D3;+0:3](-[Cl;H0;D1;+0:1]):[c:4])>>O=C1-C-C-C(=O)-N-1-[Cl;H0;D1;+0:1].[#16;a:2]:[cH;D2;+0:3]:[c:4]'], ['COc1ccc(CNc2nc(-n3ccnc3)nc3sc(C)c(C)c23)cc1OC', 'COc1ccc(CNc2nc(-n3ccnc3)nc3sc(C)c(C)c23)cc1OC*c1c[nH]cn1.COc1ccc(CNc2nc(Cl)nc3sc(C)c(C)c23)cc1OC*([#16;a:5]:[c:4]:[#7;a:3]:[c;H0;D3;+0:1](:[#7;a:2])-[n;H0;D3;+0:8]1:[c:7]:[#7;a:6]:[c:10]:[c:9]:1)>>Cl-[c;H0;D3;+0:1](:[#7;a:2]):[#7;a:3]:[c:4]:[#16;a:5].[#7;a:6]1:[c:7]:[nH;D2;+0:8]:[c:9]:[c:10]:1 COc1ccc(CNc2nc(Cl)nc3sc(C)c(C)c23)cc1OC*COc1ccc(CN)cc1OC.Cc1sc2nc(Cl)nc(Cl)c2c1C*([#16;a:5]:[c:4](:[#7;a:6]):[c:3]:[c;H0;D3;+0:1](:[#7;a:2])-[NH;D2;+0:8]-[C:7])>>Cl-[c;H0;D3;+0:1](:[#7;a:2]):[c:3]:[c:4](:[#16;a:5]):[#7;a:6].[C:7]-[NH2;D1;+0:8]'], ['COc1ccc(CNc2nc(-n3ccnc3)nc3sc(C)c(Cl)c23)cc1Cl', 'COc1ccc(CNc2nc(-n3ccnc3)nc3sc(C)c(Cl)c23)cc1Cl*c1c[nH]cn1.COc1ccc(CNc2nc(Cl)nc3sc(C)c(Cl)c23)cc1Cl*([#16;a:5]:[c:4]:[#7;a:3]:[c;H0;D3;+0:1](:[#7;a:2])-[n;H0;D3;+0:8]1:[c:7]:[#7;a:6]:[c:10]:[c:9]:1)>>Cl-[c;H0;D3;+0:1](:[#7;a:2]):[#7;a:3]:[c:4]:[#16;a:5].[#7;a:6]1:[c:7]:[nH;D2;+0:8]:[c:9]:[c:10]:1 COc1ccc(CNc2nc(Cl)nc3sc(C)c(Cl)c23)cc1Cl*COc1ccc(CN)cc1Cl.Cc1sc2nc(Cl)nc(Cl)c2c1Cl*([#16;a:5]:[c:4](:[#7;a:6]):[c:3]:[c;H0;D3;+0:1](:[#7;a:2])-[NH;D2;+0:8]-[C:7])>>Cl-[c;H0;D3;+0:1](:[#7;a:2]):[c:3]:[c:4](:[#16;a:5]):[#7;a:6].[C:7]-[NH2;D1;+0:8] Cc1sc2nc(Cl)nc(Cl)c2c1Cl*Cc1cc2c(Cl)nc(Cl)nc2s1.O=C1CCC(=O)N1Cl*([#7;a:2]:[c:3]1:[#16;a:4]:[c:5]:[c;H0;D3;+0:6](-[Cl;H0;D1;+0:1]):[c:7]:1:[c:8]:[#7;a:9])>>O=C1-C-C-C(=O)-N-1-[Cl;H0;D1;+0:1].[#7;a:2]:[c:3]1:[#16;a:4]:[c:5]:[cH;D2;+0:6]:[c:7]:1:[c:8]:[#7;a:9]'], ['COc1ccc(CNc2nc(-n3ccnc3)nc3sc(Cl)cc23)cc1OC', 'COc1ccc(CNc2nc(-n3ccnc3)nc3sc(Cl)cc23)cc1OC*c1c[nH]cn1.COc1ccc(CNc2nc(Cl)nc3sc(Cl)cc23)cc1OC*([#16;a:5]:[c:4]:[#7;a:3]:[c;H0;D3;+0:1](:[#7;a:2])-[n;H0;D3;+0:8]1:[c:7]:[#7;a:6]:[c:10]:[c:9]:1)>>Cl-[c;H0;D3;+0:1](:[#7;a:2]):[#7;a:3]:[c:4]:[#16;a:5].[#7;a:6]1:[c:7]:[nH;D2;+0:8]:[c:9]:[c:10]:1 COc1ccc(CNc2nc(Cl)nc3sc(Cl)cc23)cc1OC*COc1ccc(CN)cc1OC.Clc1nc(Cl)c2cc(Cl)sc2n1*([#16;a:5]:[c:4](:[#7;a:6]):[c:3]:[c;H0;D3;+0:1](:[#7;a:2])-[NH;D2;+0:8]-[C:7])>>Cl-[c;H0;D3;+0:1](:[#7;a:2]):[c:3]:[c:4](:[#16;a:5]):[#7;a:6].[C:7]-[NH2;D1;+0:8] Clc1nc(Cl)c2cc(Cl)sc2n1*O=C1CCC(=O)N1Cl.Clc1nc(Cl)c2ccsc2n1*([#16;a:2]:[c;H0;D3;+0:3](-[Cl;H0;D1;+0:1]):[c:4])>>O=C1-C-C-C(=O)-N-1-[Cl;H0;D1;+0:1].[#16;a:2]:[cH;D2;+0:3]:[c:4]']]\n",
      "Number of new molecules: 5\n",
      "========computing qed of molecule0\n",
      "========computing qed of molecule1\n",
      "========computing qed of molecule2\n",
      "========computing qed of molecule3\n",
      "========computing qed of molecule4\n",
      "========Updating training set\n",
      "X_train shape before update: torch.Size([18073, 50])\n",
      "y_train shape before update: torch.Size([18073, 1])\n",
      "X_train shape after update: torch.Size([18074, 50])\n",
      "y_train shape after update: torch.Size([18074, 1])\n",
      "Writing to file: ./Results/42_qed.txt\n",
      "--- Starting Iteration 2 ---\n",
      "========shape:  torch.Size([18074, 50]) torch.Size([18074, 1]) torch.Size([2008, 50]) torch.Size([2008, 1])\n",
      "lr:  0.001\n",
      "Model -- Epoch 0 error on validation set: 0.1301, r2 on validation set: -1.9024\n",
      "Model -- Epoch 100 error on validation set: 0.0435, r2 on validation set: 0.0284\n",
      "Model -- Epoch 200 error on validation set: 0.0433, r2 on validation set: 0.0333\n",
      "Model -- Epoch 300 error on validation set: 0.0436, r2 on validation set: 0.0262\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      2\u001b[39m optimizer = binary_vae_utils.MoleculeOptimizer(bvae_model=model,surrogate_model=FM_surrogate,X_train=X_train,y_train=y_train,X_test=X_test,y_test=y_test,qubo_solver=gurobi_solver)\n\u001b[32m      4\u001b[39m start_time = time.time()\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m logging.info(\u001b[33m\"\u001b[39m\u001b[33mRunning Time: \u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[33m\"\u001b[39m % (time.time() - start_time))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/brxngenerator/binary_vae_utils.py:311\u001b[39m, in \u001b[36mMoleculeOptimizer.optimize\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    308\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m--- Starting Iteration \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.iteration\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    310\u001b[39m \u001b[38;5;66;03m# 1. train surrogate model\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m311\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msurrogate\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    313\u001b[39m solutions, energies = \u001b[38;5;28mself\u001b[39m.solver.solve(\u001b[38;5;28mself\u001b[39m.surrogate.model)\n\u001b[32m    316\u001b[39m \u001b[38;5;66;03m# 4. update the dataset \u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/brxngenerator/binary_vae_utils.py:200\u001b[39m, in \u001b[36mFactorizationMachineSurrogate.train\u001b[39m\u001b[34m(self, X_train, y_train, X_test, y_test)\u001b[39m\n\u001b[32m    198\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(MAX_EPOCH):\n\u001b[32m    199\u001b[39m     model.train()\n\u001b[32m--> \u001b[39m\u001b[32m200\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_y\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdataloader_train\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_y\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_x\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_y\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Ensure data is on the GPU\u001b[39;49;00m\n\u001b[32m    202\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/brxngenerator/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:733\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    736\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    739\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/brxngenerator/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:789\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    787\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    788\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m789\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    790\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    791\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/brxngenerator/.venv/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28mself\u001b[39m.dataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import binary_vae_utils\n",
    "optimizer = binary_vae_utils.MoleculeOptimizer(bvae_model=model,surrogate_model=FM_surrogate,X_train=X_train,y_train=y_train,X_test=X_test,y_test=y_test,qubo_solver=gurobi_solver)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "optimizer.optimize()\n",
    "\n",
    "logging.info(\"Running Time: %f\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7d7f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New QUBO solution found with objective 0.0\n",
      "New QUBO solution found with objective -1.0\n",
      "New QUBO solution found with objective -4.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from gurobi_optimods.qubo import solve_qubo\n",
    "\n",
    "Q = np.array([[0, -1, -2], [0, -3, 3], [0, 0, 2]])\n",
    "\n",
    "# weights = [-3, 2, -1, -2, 3]\n",
    "# row = [1, 2, 0, 0, 1]\n",
    "# col = [1, 2, 1, 2, 2]\n",
    "# Q = sp.coo_array((weights, (row, col)), shape=(3, 3))\n",
    "\n",
    "result = solve_qubo(Q)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brxngenerator",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
