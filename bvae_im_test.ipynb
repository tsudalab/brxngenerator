{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append('./rxnft_vae')\n",
    "\n",
    "import rdkit\n",
    "from rdkit.Chem import QED, Descriptors, rdmolops\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import math, random, sys\n",
    "from optparse import OptionParser\n",
    "import pickle as pickle\n",
    "import yaml\n",
    "from rxnft_vae.reaction_utils import get_mol_from_smiles, get_smiles_from_mol, read_multistep_rxns, get_template_order, \\\n",
    "    get_qed_score, get_clogp_score\n",
    "from rxnft_vae.reaction import ReactionTree, extract_starting_reactants, StartingReactants, Templates, \\\n",
    "    extract_templates, stats\n",
    "from rxnft_vae.fragment import FragmentVocab, FragmentTree, FragmentNode, can_be_decomposed\n",
    "from rxnft_vae.vae import FTRXNVAE, set_batch_nodeID, bFTRXNVAE\n",
    "from rxnft_vae.mpn import MPN, PP, Discriminator\n",
    "import random\n",
    "import rxnft_vae.sascorer as sascorer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from amplify import BinaryMatrix, BinaryPoly, gen_symbols, sum_poly\n",
    "from amplify import decode_solution, Solver\n",
    "from amplify.client import FixstarsClient\n",
    "from amplify.client.ocean import DWaveSamplerClient\n",
    "import logging\n",
    "import time\n",
    "\n",
    "UPDATE_ITER = 1\n",
    "\n",
    "metric = \"qed\"\n",
    "\n",
    "class TorchFM(nn.Module):\n",
    "\n",
    "    def __init__(self, n=None, k=None):\n",
    "        super().__init__()\n",
    "        self.V = nn.Parameter(torch.randn(n, k), requires_grad=True)\n",
    "        self.lin = nn.Linear(n, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out_1 = torch.matmul(x, self.V).pow(2).sum(1, keepdim=True)  # S_1^2\n",
    "        out_2 = torch.matmul(x.pow(2), self.V.pow(2)).sum(1, keepdim=True)  # S_2\n",
    "\n",
    "        out_inter = 0.5 * (out_1 - out_2)\n",
    "        out_lin = self.lin(x)\n",
    "        out = out_inter + out_lin\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class MolData(Dataset):\n",
    "\n",
    "    def __init__(self, binary, targets):\n",
    "        self.binary = binary\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.binary)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.binary[index], self.targets[index]\n",
    "\n",
    "\n",
    "class RandomBinaryData(Dataset):\n",
    "\n",
    "    def __init__(self, binary):\n",
    "        self.binary = binary\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.binary)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.binary[index]\n",
    "\n",
    "\n",
    "class bVAE_IM(object):\n",
    "    def __init__(self, bvae_model=None, smiles=None, targets=None, seed=0, n_sample=1):\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.bvae_model = bvae_model.to(self.device)\n",
    "        self.train_smiles = smiles\n",
    "        self.train_targets = targets\n",
    "        \n",
    "        self.random_seed = seed\n",
    "        self.X_train = None\n",
    "        self.y_train = None\n",
    "        self.X_test = None\n",
    "        self.y_test = None\n",
    "        if self.random_seed is not None:\n",
    "            seed_all(self.random_seed)\n",
    "\n",
    "        self.n_sample = n_sample  # configs['opt']['n_sample']\n",
    "        self.sleep_count = 0\n",
    "\n",
    "    def decode_many_times(self, latent):\n",
    "\n",
    "        prob_decode = True\n",
    "        binary_size = self.bvae_model.binary_size\n",
    "\n",
    "        product_list = []\n",
    "        for i in range(10):\n",
    "            if len(product_list) > 5:\n",
    "                break\n",
    "            latent_new = latent\n",
    "            # latent_new = torch.cat([latent, torch.randint(0, 2, (latent.shape[0], latent.shape[1] * 2 - latent.shape[1]))], dim=1).to(self.device)\n",
    "            # print(\"latent_new shape\", latent_new.shape)\n",
    "            binary = F.one_hot(latent_new.long(), num_classes=2).float().to(self.device)\n",
    "            binary = binary.view(1, -1)\n",
    "            ft_mean = binary[:, :binary_size * 2]\n",
    "            rxn_mean = binary[:, binary_size * 2:]\n",
    "            # print(\"ft_mean shape\", ft_mean.shape)\n",
    "            # print(\"rxn_mean shape\", rxn_mean.shape)\n",
    "            generated_tree = self.bvae_model.fragment_decoder.decode(ft_mean, prob_decode)\n",
    "            # print(\"generated_tree shape\", generated_tree.shape)\n",
    "            g_encoder_output, g_root_vec = self.bvae_model.fragment_encoder([generated_tree])\n",
    "            # print(\"g_encoder_output shape\", g_encoder_output.shape)\n",
    "            # print(\"g_root_vec shape\", g_root_vec.shape)\n",
    "            product, reactions = self.bvae_model.rxn_decoder.decode(rxn_mean, g_encoder_output, prob_decode)\n",
    "            if product != None:\n",
    "                product_list.append([product, reactions])\n",
    "        if len(product_list) == 0:\n",
    "            return None\n",
    "        else:\n",
    "            return product_list\n",
    "\n",
    "    def optimize(self, X_train, y_train, X_test, y_test, configs):\n",
    "        self.X_train = X_train.to(self.device)\n",
    "        self.y_train = y_train.to(self.device)\n",
    "        self.X_test = X_test.to(self.device)\n",
    "        self.y_test = y_test.to(self.device)\n",
    "        \n",
    "        \n",
    "        n_opt = 100  # configs['opt']['num_end']\n",
    "        self.train_binary = torch.vstack((X_train, X_test))\n",
    "        self.n_binary = self.train_binary.shape[1]\n",
    "\n",
    "        self.valid_smiles = []\n",
    "        self.new_features = []\n",
    "        self.full_rxn_strs = []\n",
    "\n",
    "\n",
    "        self.end_cond = configs['opt']['end_cond']\n",
    "        if self.end_cond not in [0, 1, 2]:\n",
    "            raise ValueError(\"end_cond should be 0, 1 or 2.\")\n",
    "        if self.end_cond == 2:\n",
    "            n_opt = 100  # n_opt is patience in this condition. When patience exceeds 100, exhaustion searching ends.\n",
    "\n",
    "        self.results_smiles = []\n",
    "        self.results_binary = []\n",
    "        self.results_scores = []\n",
    "\n",
    "\n",
    "        client = FixstarsClient()\n",
    "        client.token = configs['opt']['client_token']\n",
    "        client.parameters.timeout = 1000\n",
    "\n",
    "        solver = Solver(client)\n",
    "\n",
    "        self.iteration = 0\n",
    "\n",
    "        while self.iteration < n_opt:\n",
    "            qubo = self._build_qubo(configs)\n",
    "\n",
    "            solution, energy = self._solve_qubo(qubo=qubo,\n",
    "                                                qubo_solver=solver)\n",
    "\n",
    "            self._update(solution=solution,\n",
    "                         energy=energy)\n",
    "\n",
    "        result_save_dir = configs['opt']['output']\n",
    "        if not os.path.exists(result_save_dir):\n",
    "            os.mkdir(result_save_dir)\n",
    "\n",
    "        with open((os.path.join(result_save_dir, \"%s_smiles.pkl\" % configs['opt']['prop'])), \"wb\") as f:\n",
    "            pickle.dump(self.results_smiles, f)\n",
    "        with open((os.path.join(result_save_dir, \"%s_scores.pkl\" % configs['opt']['prop'])), \"wb\") as f:\n",
    "            pickle.dump(self.results_scores, f)\n",
    "\n",
    "        logging.info(\"Sleeped for %d minutes...\" % self.sleep_count)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def _build_qubo(self, configs):\n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model = TorchFM(self.n_binary, configs['opt']['factor_num']).to(device)\n",
    "        \n",
    "        for param in model.parameters():\n",
    "            if param.dim() == 1:\n",
    "                nn.init.constant_(param, 0)  # bias\n",
    "            else:\n",
    "                nn.init.uniform_(param, -configs['opt']['param_init'], configs['opt']['param_init'])  # weights\n",
    "\n",
    "        print('========shape: ', self.X_train.shape, self.y_train.shape, self.X_test.shape, self.y_test.shape)\n",
    "        dataset_train = MolData(self.X_train, self.y_train)\n",
    "        dataloader_train = DataLoader(dataset=dataset_train,\n",
    "                                      batch_size=configs['opt']['batch_size'],\n",
    "                                      shuffle=True)\n",
    "        dataset_valid = MolData(self.X_test, self.y_test)\n",
    "        dataloader_valid = DataLoader(dataset=dataset_valid,\n",
    "                                      batch_size=configs['opt']['batch_size'],\n",
    "                                      shuffle=False)\n",
    "\n",
    "        print('lr: ', configs['opt']['lr'])\n",
    "        optimizer = torch.optim.Adam(model.parameters(),\n",
    "                                     lr=configs['opt']['lr'],\n",
    "                                     weight_decay=configs['opt']['decay_weight'])\n",
    "        criterion = nn.MSELoss()\n",
    "\n",
    "        lowest_error = float('inf')\n",
    "        best_epoch = 0\n",
    "\n",
    "        for epoch in range(configs['opt']['maxepoch']):\n",
    "            model.train()\n",
    "            for batch_x, batch_y in dataloader_train:\n",
    "                batch_x, batch_y = batch_x.to(device), batch_y.to(device)  # Ensure data is on the GPU\n",
    "                optimizer.zero_grad()\n",
    "                out = model(batch_x)\n",
    "                loss = criterion(out, batch_y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                y_hat_test = []\n",
    "                for batch_x, _ in dataloader_valid:\n",
    "                    batch_x = batch_x.to(device)  # Ensure data is on the GPU\n",
    "                    valid = model(batch_x)\n",
    "                    y_hat_test.append(valid)\n",
    "                y_hat_test = torch.cat(y_hat_test)\n",
    "\n",
    "                epoch_error = criterion(self.y_test.to(device), y_hat_test)  # Ensure target is on the GPU\n",
    "                r2_test = r2_score(self.y_test.cpu().numpy(), y_hat_test.cpu().numpy())\n",
    "                epoch_error = epoch_error.detach().cpu().numpy()\n",
    "                if epoch % 100 == 0:\n",
    "                    print(\"Model -- Epoch %d error on validation set: %.4f, r2 on validation set: %.4f\" % (epoch, epoch_error, r2_test))\n",
    "\n",
    "                if epoch_error < lowest_error:\n",
    "                    torch.save(model.state_dict(),\n",
    "                               os.path.join(configs['opt']['cache'],\n",
    "                                            \"fm_model-%s-%s-dim%d-seed%d-end%d\" % (\n",
    "                                                configs['opt']['prop'],\n",
    "                                                configs['opt']['client'],\n",
    "                                                self.n_binary,\n",
    "                                                self.random_seed,\n",
    "                                                self.end_cond)))\n",
    "                    lowest_error = epoch_error\n",
    "                    best_epoch = epoch\n",
    "\n",
    "                if epoch > best_epoch + configs['opt']['patience']:\n",
    "                    print(\"Model -- Epoch %d has lowest error!\" % (best_epoch))\n",
    "                    break\n",
    "\n",
    "        y_hat_test = y_hat_test.unsqueeze(1).detach().cpu().numpy()\n",
    "        y_test = self.y_test.detach().cpu().numpy()\n",
    "        print(y_hat_test.shape, y_test.shape)\n",
    "        model.load_state_dict(torch.load(\n",
    "            os.path.join(configs['opt']['cache'],\n",
    "                         \"fm_model-%s-%s-dim%d-seed%d-end%d\" % (\n",
    "                             configs['opt']['prop'],\n",
    "                             configs['opt']['client'],\n",
    "                             self.n_binary,\n",
    "                             self.random_seed,\n",
    "                             self.end_cond)))\n",
    "        )\n",
    "\n",
    "        for p in model.parameters():\n",
    "            if tuple(p.shape) == (self.n_binary, configs['opt']['factor_num']):\n",
    "                Vi_f = p.to(\"cpu\").detach().numpy()\n",
    "            elif tuple(p.shape) == (1, self.n_binary):\n",
    "                Wi = p.to(\"cpu\").detach().numpy()\n",
    "            elif tuple(p.shape) == (1,):\n",
    "                W0 = p.to(\"cpu\").detach().numpy()\n",
    "\n",
    "        q = gen_symbols(BinaryPoly, self.n_binary)\n",
    "        f_E = sum_poly(configs['opt']['factor_num'], lambda f: (\n",
    "                    (sum_poly(self.n_binary, lambda i: Vi_f[i][f] * q[i])) ** 2 - sum_poly(self.n_binary,\n",
    "                                                                                           lambda i: Vi_f[i][f] ** 2 *\n",
    "                                                                                                     q[i] ** 2))) / 2 \\\n",
    "              + sum_poly(self.n_binary, lambda i: Wi[0][i] * q[i]) \\\n",
    "              + W0[0]\n",
    "        qubo = (q, f_E)\n",
    "\n",
    "        return qubo\n",
    "\n",
    "    def _solve_qubo(self,\n",
    "                    qubo,\n",
    "                    qubo_solver):\n",
    "\n",
    "        if isinstance(qubo, tuple):\n",
    "            q, qubo = qubo\n",
    "\n",
    "        solved = False\n",
    "        while not solved:\n",
    "            try:\n",
    "                result = qubo_solver.solve(qubo)\n",
    "                solved = True\n",
    "            except RuntimeError as e:  # retry after 60s if connection to the solver fails..\n",
    "                time.sleep(60)\n",
    "                self.sleep_count += 1\n",
    "\n",
    "        sols = []\n",
    "        sol_E = []\n",
    "        for sol in result:  # Iterate over multiple solutions\n",
    "            if isinstance(qubo, BinaryMatrix):\n",
    "                solution = [sol.values[i] for i in range(self.n_binary)]\n",
    "            elif isinstance(qubo, BinaryPoly):\n",
    "                solution = decode_solution(q, sol.values)\n",
    "            else:\n",
    "                raise ValueError(\"qubo type unknown!\")\n",
    "            sols.append(solution)\n",
    "            sol_E.append(sol.energy)\n",
    "        return np.array(sols), np.array(sol_E).astype(np.float32)\n",
    "    def _update(self,\n",
    "            solution,\n",
    "            energy):\n",
    "\n",
    "        if self.end_cond == 0:\n",
    "            self.iteration += 1\n",
    "\n",
    "        binary_new = torch.from_numpy(solution).to(torch.float)\n",
    "        print('========binary_new shape')\n",
    "        print(binary_new.shape)\n",
    "        print(\"shape of binary_new\", binary_new.shape)\n",
    "        res = self.decode_many_times(binary_new)\n",
    "        print('========res')\n",
    "        print(res)\n",
    "\n",
    "        if res is None:\n",
    "            print('========res is None')\n",
    "            return\n",
    "\n",
    "        preLength = 0\n",
    "        new_smiles = []\n",
    "        new_rxn_strs = []\n",
    "\n",
    "        for re in res:\n",
    "            smiles = re[0]\n",
    "            if len(re[1].split(\" \")) > 0 and smiles not in self.valid_smiles:\n",
    "                preLength += 1\n",
    "                self.valid_smiles.append(smiles)\n",
    "                self.new_features.append(latent)\n",
    "                self.full_rxn_strs.append(re[1])\n",
    "                new_smiles.append(smiles)\n",
    "                new_rxn_strs.append(re[1])\n",
    "\n",
    "        if preLength == 0:\n",
    "            print(\"No new valid molecules generated.\")\n",
    "            return\n",
    "\n",
    "        print(\"Number of new molecules:\", preLength)\n",
    "\n",
    "        scores = []\n",
    "        b_valid_smiles = []\n",
    "        b_full_rxn_strs = []\n",
    "        b_scores = []\n",
    "\n",
    "        for i in range(preLength):\n",
    "            mol = rdkit.Chem.MolFromSmiles(new_smiles[i])\n",
    "            if mol is None:\n",
    "                continue\n",
    "            if metric == \"logp\":\n",
    "                print('========computing logp of molecule{}'.format(i))\n",
    "                logP_values = np.loadtxt('./data/logP_values.txt')\n",
    "                SA_scores = np.loadtxt('./data/SA_scores.txt')\n",
    "                cycle_scores = np.loadtxt('./data/cycle_scores.txt')\n",
    "\n",
    "                logp_m = np.mean(logP_values)\n",
    "                logp_s = np.std(logP_values)\n",
    "\n",
    "                sascore_m = np.mean(SA_scores)\n",
    "                sascore_s = np.std(SA_scores)\n",
    "\n",
    "                cycle_m = np.mean(cycle_scores)\n",
    "                cycle_s = np.std(cycle_scores)\n",
    "                smiles = new_smiles[i]\n",
    "                score = get_clogp_score(smiles, logp_m, logp_s, sascore_m, sascore_s, cycle_m, cycle_s)\n",
    "                scores.append(-score)\n",
    "\n",
    "            elif metric == \"qed\":\n",
    "                print('========computing qed of molecule{}'.format(i))\n",
    "                score = QED.qed(mol)\n",
    "                scores.append(-score)\n",
    "            else:\n",
    "                raise ValueError(\"Unsupported metric: {}\".format(metric))\n",
    "            b_valid_smiles.append(new_smiles[i])\n",
    "            b_full_rxn_strs.append(new_rxn_strs[i])\n",
    "\n",
    "        if len(scores) >= 1:\n",
    "            b_scores = scores.copy()\n",
    "            avg_score = np.mean(scores)\n",
    "            training_score = [avg_score]\n",
    "        else:\n",
    "            print(\"No valid scores calculated.\")\n",
    "            return\n",
    "\n",
    "        if len(binary_new) > 0:\n",
    "            print('========Updating training set')\n",
    "            print('X_train shape before update:', self.X_train.shape)\n",
    "            print('y_train shape before update:', self.y_train.shape)\n",
    "            self.X_train = np.concatenate([self.X_train, binary_new], 0)\n",
    "            self.y_train = np.concatenate([self.y_train, np.array(training_score)[:, None]], 0)\n",
    "            self.y_train = self.y_train.astype(np.float32)\n",
    "            \n",
    "            print('X_train shape after update:', self.X_train.shape)\n",
    "            print('y_train shape after update:', self.y_train.shape)\n",
    "\n",
    "        TaskID = os.environ.get(\"TaskID\", \"default_task\")\n",
    "\n",
    "        if metric == \"logp\":\n",
    "            filename = \"./Results/\" + TaskID + \"_logp.txt\"\n",
    "        elif metric == \"qed\":\n",
    "            filename = \"./Results/\" + TaskID + \"_qed.txt\"\n",
    "\n",
    "        print(\"Writing to file:\", filename)\n",
    "        with open(filename, \"a\") as writer:\n",
    "            for i in range(len(b_valid_smiles)):\n",
    "                line = \" \".join([b_valid_smiles[i], b_full_rxn_strs[i], str(b_scores[i])])\n",
    "                writer.write(line + \"\\n\")\n",
    "\n",
    "        assert self.X_train.shape[0] == self.y_train.shape[0]\n",
    "\n",
    "        return\n",
    "\n",
    "def main(X_train, y_train, X_test, y_test, smiles, targets, model, parameters, configs, metric, seed):\n",
    "    X_train = torch.Tensor(X_train)\n",
    "    y_train = torch.Tensor(y_train)\n",
    "    X_test = torch.Tensor(X_test)\n",
    "    y_test = torch.Tensor(y_test)\n",
    "\n",
    "    optimizer = bVAE_IM(smiles=smiles, targets=targets, bvae_model=model, seed=seed)\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    optimizer.optimize(X_train, y_train, X_test, y_test, configs)\n",
    "\n",
    "    logging.info(\"Running Time: %f\" % (time.time() - start_time))\n",
    "\n",
    "\n",
    "def seed_all(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden size: 300 latent_size: 100 depth: 2\n",
      "loading data.....\n",
      "size of reactant dic: 9766\n",
      "size of template dic: 5567\n",
      "size of fgm_trees: 20080\n",
      "size of rxn_trees: 20080\n",
      "size of fragment dic: 273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gzou/miniforge3/envs/newbrx/lib/python3.9/site-packages/torch/nn/_reduction.py:51: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/gzou/miniforge3/envs/newbrx/lib/python3.9/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished loading model...\n",
      "number of samples: 20080\n",
      "num of samples: 20080\n",
      "========start to compute all scores\n",
      "=================== 20080\n",
      "(18072, 100) (2008, 100) (18072, 1) (2008, 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "hidden_size = 300\n",
    "latent_size = 100\n",
    "depth = 2\n",
    "data_filename = \"/home/gzou/fitcheck/newnnn/brxngenerator-master/data/data.txt\"\n",
    "w_save_path = \"/home/gzou/fitcheck/newnnn/brxngenerator-master/weights/hidden_size_300_latent_size_100_depth_2_beta_1.0_lr_0.001/bvae_iter-30-with.npy\"\n",
    "metric = \"qed\"\n",
    "seed = 1\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"hidden size:\", hidden_size, \"latent_size:\", latent_size, \"depth:\", depth)\n",
    "print(\"loading data.....\")\n",
    "routes, scores = read_multistep_rxns(data_filename)\n",
    "rxn_trees = [ReactionTree(route) for route in routes]\n",
    "molecules = [rxn_tree.molecule_nodes[0].smiles for rxn_tree in rxn_trees]\n",
    "reactants = extract_starting_reactants(rxn_trees)\n",
    "templates, n_reacts = extract_templates(rxn_trees)\n",
    "reactantDic = StartingReactants(reactants)\n",
    "templateDic = Templates(templates, n_reacts)\n",
    "\n",
    "print(\"size of reactant dic:\", reactantDic.size())\n",
    "print(\"size of template dic:\", templateDic.size())\n",
    "\n",
    "n_pairs = len(routes)\n",
    "ind_list = [i for i in range(n_pairs)]\n",
    "\n",
    "fgm_trees = []\n",
    "valid_id = []\n",
    "for i in ind_list:\n",
    "    try:\n",
    "        fgm_trees.append(FragmentTree(rxn_trees[i].molecule_nodes[0].smiles))\n",
    "        valid_id.append(i)\n",
    "    except Exception as e:\n",
    "        # print(e)\n",
    "        continue\n",
    "rxn_trees = [rxn_trees[i] for i in valid_id]\n",
    "\n",
    "print(\"size of fgm_trees:\", len(fgm_trees))\n",
    "print(\"size of rxn_trees:\", len(rxn_trees))\n",
    "data_pairs = []\n",
    "for fgm_tree, rxn_tree in zip(fgm_trees, rxn_trees):\n",
    "    data_pairs.append((fgm_tree, rxn_tree))\n",
    "cset = set()\n",
    "for fgm_tree in fgm_trees:\n",
    "    for node in fgm_tree.nodes:\n",
    "        cset.add(node.smiles)\n",
    "cset = list(cset)\n",
    "fragmentDic = FragmentVocab(cset)\n",
    "\n",
    "print(\"size of fragment dic:\", fragmentDic.size())\n",
    "\n",
    "\n",
    "mpn = MPN(hidden_size, depth)\n",
    "model = bFTRXNVAE(fragmentDic, reactantDic, templateDic, hidden_size, latent_size, depth, device,\n",
    "                    fragment_embedding=None, reactant_embedding=None, template_embedding=None).to(device)\n",
    "checkpoint = torch.load(w_save_path, map_location=device)\n",
    "model.load_state_dict(checkpoint)\n",
    "print(\"finished loading model...\")\n",
    "\n",
    "print(\"number of samples:\", len(data_pairs))\n",
    "data_pairs = data_pairs\n",
    "latent_list = []\n",
    "score_list = []\n",
    "print(\"num of samples:\", len(rxn_trees))\n",
    "latent_list = []\n",
    "score_list = []\n",
    "print('========start to compute all scores')\n",
    "if metric == \"qed\":\n",
    "    for i, data_pair in enumerate(data_pairs):\n",
    "        latent = model.encode([data_pair])\n",
    "        latent_list.append(latent[0])\n",
    "        rxn_tree = data_pair[1]\n",
    "        smiles = rxn_tree.molecule_nodes[0].smiles\n",
    "        score_list.append(get_qed_score(smiles))\n",
    "if metric == \"logp\":\n",
    "    logP_values = np.loadtxt('./data/logP_values.txt')\n",
    "    SA_scores = np.loadtxt('./data/SA_scores.txt')\n",
    "    cycle_scores = np.loadtxt('./data/cycle_scores.txt')\n",
    "\n",
    "    logp_m = np.mean(logP_values)\n",
    "    logp_s = np.std(logP_values)\n",
    "\n",
    "    sascore_m = np.mean(SA_scores)\n",
    "    sascore_s = np.std(SA_scores)\n",
    "\n",
    "    cycle_m = np.mean(cycle_scores)\n",
    "    cycle_s = np.std(cycle_scores)\n",
    "    for i, data_pair in enumerate(data_pairs):\n",
    "        latent = model.encode([data_pair])\n",
    "        latent_list.append(latent[0])\n",
    "        rxn_tree = data_pair[1]\n",
    "        smiles = rxn_tree.molecule_nodes[0].smiles\n",
    "        score_list.append(get_clogp_score(smiles, logp_m, logp_s, sascore_m, sascore_s, cycle_m, cycle_s))\n",
    "latents = torch.stack(latent_list, dim=0)\n",
    "scores = np.array(score_list)\n",
    "scores = scores.reshape((-1, 1))\n",
    "# move to cpu first\n",
    "latents = latents.detach().cpu().numpy()\n",
    "n = latents.shape[0]\n",
    "print('===================', n)\n",
    "permutation = np.random.choice(n, n, replace=False)\n",
    "X_train = latents[permutation, :][0: int(np.round(0.9 * n)), :]\n",
    "X_test = latents[permutation, :][int(np.round(0.9 * n)):, :]\n",
    "y_train = -scores[permutation][0: int(np.round(0.9 * n))]\n",
    "y_test = -scores[permutation][int(np.round(0.9 * n)):]\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "if metric == \"logp\":\n",
    "    parameters = [logp_m, logp_s, sascore_m, sascore_s, cycle_m, cycle_s]\n",
    "else:\n",
    "    parameters = []\n",
    "\n",
    "with open('config/config.yaml', 'r') as f:\n",
    "    configs = yaml.safe_load(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========shape:  torch.Size([18072, 100]) torch.Size([18072, 1]) torch.Size([2008, 100]) torch.Size([2008, 1])\n",
      "lr:  0.001\n",
      "Model -- Epoch 0 error on validation set: 0.1076, r2 on validation set: -1.4074\n",
      "Model -- Epoch 100 error on validation set: 0.0441, r2 on validation set: 0.0138\n",
      "Model -- Epoch 200 error on validation set: 0.0442, r2 on validation set: 0.0120\n",
      "Model -- Epoch 300 error on validation set: 0.0440, r2 on validation set: 0.0164\n",
      "Model -- Epoch 400 error on validation set: 0.0436, r2 on validation set: 0.0242\n",
      "Model -- Epoch 500 error on validation set: 0.0443, r2 on validation set: 0.0095\n",
      "Model -- Epoch 600 error on validation set: 0.0442, r2 on validation set: 0.0105\n",
      "Model -- Epoch 700 error on validation set: 0.0440, r2 on validation set: 0.0147\n",
      "Model -- Epoch 466 has lowest error!\n",
      "(2008, 1, 1) (2008, 1)\n",
      "========binary_new shape\n",
      "torch.Size([1, 100])\n",
      "shape of binary_new torch.Size([1, 100])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gzou/miniforge3/envs/newbrx/lib/python3.9/site-packages/torch/nn/modules/module.py:1739: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========res\n",
      "None\n",
      "========res is None\n",
      "========shape:  torch.Size([18072, 100]) torch.Size([18072, 1]) torch.Size([2008, 100]) torch.Size([2008, 1])\n",
      "lr:  0.001\n",
      "Model -- Epoch 0 error on validation set: 0.0572, r2 on validation set: -0.2790\n",
      "Model -- Epoch 100 error on validation set: 0.0436, r2 on validation set: 0.0244\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmolecules\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mscores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfigs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[20], line 439\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(X_train, y_train, X_test, y_test, smiles, targets, model, parameters, configs, metric, seed)\u001b[0m\n\u001b[1;32m    435\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m bVAE_IM(smiles\u001b[38;5;241m=\u001b[39msmiles, targets\u001b[38;5;241m=\u001b[39mtargets, bvae_model\u001b[38;5;241m=\u001b[39mmodel, seed\u001b[38;5;241m=\u001b[39mseed)\n\u001b[1;32m    437\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 439\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfigs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    441\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning Time: \u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time))\n",
      "Cell \u001b[0;32mIn[20], line 165\u001b[0m, in \u001b[0;36mbVAE_IM.optimize\u001b[0;34m(self, X_train, y_train, X_test, y_test, configs)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miteration \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miteration \u001b[38;5;241m<\u001b[39m n_opt:\n\u001b[0;32m--> 165\u001b[0m     qubo \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_qubo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfigs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m     solution, energy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_solve_qubo(qubo\u001b[38;5;241m=\u001b[39mqubo,\n\u001b[1;32m    168\u001b[0m                                         qubo_solver\u001b[38;5;241m=\u001b[39msolver)\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update(solution\u001b[38;5;241m=\u001b[39msolution,\n\u001b[1;32m    171\u001b[0m                  energy\u001b[38;5;241m=\u001b[39menergy)\n",
      "Cell \u001b[0;32mIn[20], line 230\u001b[0m, in \u001b[0;36mbVAE_IM._build_qubo\u001b[0;34m(self, configs)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    229\u001b[0m     y_hat_test \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 230\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch_x, _ \u001b[38;5;129;01min\u001b[39;00m dataloader_valid:\n\u001b[1;32m    231\u001b[0m         batch_x \u001b[38;5;241m=\u001b[39m batch_x\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m# Ensure data is on the GPU\u001b[39;00m\n\u001b[1;32m    232\u001b[0m         valid \u001b[38;5;241m=\u001b[39m model(batch_x)\n",
      "File \u001b[0;32m~/miniforge3/envs/newbrx/lib/python3.9/site-packages/torch/utils/data/dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    714\u001b[0m ):\n",
      "File \u001b[0;32m~/miniforge3/envs/newbrx/lib/python3.9/site-packages/torch/utils/data/dataloader.py:764\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    763\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 764\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    766\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniforge3/envs/newbrx/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/miniforge3/envs/newbrx/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[20], line 65\u001b[0m, in \u001b[0;36mMolData.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary[index], \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtargets\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main(X_train, y_train, X_test, y_test, molecules, -scores, model, parameters, configs, metric, seed)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newbrx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
