# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project Overview

This is **brxngenerator**, a binary-version implementation of Cascade-VAE for molecular synthesis route generation and optimization. The project combines:

- **Binary Variational Autoencoder (B-VAE)**: Neural architecture with binary latent space for molecular fragment encoding
- **Error-Correcting Codes (ECC)**: Optional repetition codes for improved generation quality and robustness
- **Ising Machine Optimization**: Uses Gurobi solver for QUBO optimization of molecular properties
- **Molecular Property Optimization**: Focuses on QED (drug-likeness) and logP optimization

## Core Architecture

### Primary Components

1. **B-VAE Model** (`rxnft_vae/vae.py`):
   - `bFTRXNVAE`: Binary fragment-tree reaction VAE with binary latent vectors
   - Encodes molecular fragments into binary latent space (typically 100-300 dimensions)
   - Trained on reaction route datasets for molecular synthesis

2. **ECC Module** (`rxnft_vae/ecc.py`):
   - `RepetitionECC`: Error-correcting codes with R=2,3 repetition factors
   - Majority-vote decoding corrects up to âŒŠ(R-1)/2âŒ‹ errors per group
   - Optional feature (disabled by default) for improved generation quality

3. **Optimization Pipeline** (`mainstream.py`):
   - Loads pre-trained B-VAE models
   - Uses Factorization Machine as surrogate model
   - Employs Gurobi QUBO solver for binary optimization
   - Molecular property targets: QED score, logP values

4. **Binary VAE Utilities** (`binary_vae_utils.py`):
   - `TorchFM`: Factorization Machine implementation
   - `GurobiQuboSolver`: Gurobi-based QUBO optimization
   - `MoleculeOptimizer`: Main optimization orchestrator
   - ECC-aware dataset preparation functions

### Key Modules

- **Fragment Processing** (`rxnft_vae/fragment.py`): Molecular fragment tree construction
- **Reaction Processing** (`rxnft_vae/reaction.py`): Reaction tree parsing and template extraction
- **Neural Networks** (`rxnft_vae/ftencoder.py`, `ftdecoder.py`): Fragment-tree encoder/decoder
- **Evaluation** (`rxnft_vae/evaluate.py`): ECC-aware evaluator with improved latent generation
- **Configuration** (`config.py`, `config/config.yaml`): Centralized settings and hyperparameters

## âš ï¸ é‡è¦ï¼šå‘½ä»¤è¡Œæ¥å£æ›´æ–°

**æ–°çš„trainvae.pyæ¥å£ï¼ˆ2025å¹´æ›´æ–°ï¼‰:**
```bash
# âœ… æ–°çš„æ­£ç¡®ç”¨æ³•
python trainvae.py -n 0 --subset 2000 --ecc-type repetition --ecc-R 2

# âŒ æ—§çš„ç”¨æ³•å·²ä¸å†æ”¯æŒ
python trainvae.py -w 200 -l 100 -d 2 -v "./weights/data.txt_fragmentvocab.txt" -t "./data/data.txt"
```

**å…³é”®å˜æ›´ï¼š**
- `trainvae.py`ç°åœ¨åªæ¥å—`-n`(å‚æ•°é›†ç´¢å¼•)ã€`--ecc-type`ã€`--ecc-R`ã€`--subset`å‚æ•°
- æ•°æ®è·¯å¾„å’Œè¯æ±‡è¡¨è·¯å¾„å·²ç¡¬ç¼–ç ï¼š`./data/data.txt`å’Œ`./weights/data.txt_fragmentvocab.txt`  
- æ¨¡å‹å‚æ•°é€šè¿‡é¢„å®šä¹‰çš„å‚æ•°é›†é€‰æ‹©ï¼ˆ0-7ï¼‰ï¼Œä¸å†æ”¯æŒå•ç‹¬æŒ‡å®š`-w`ã€`-l`ã€`-d`ç­‰
- ECCå‚æ•°å¿…é¡»æ»¡è¶³å…¼å®¹æ€§è¦æ±‚ï¼š`latent_size % ecc_R == 0`

## Development Commands

### Model Training
```bash
# Standard B-VAE training (uses predefined parameter sets)
python trainvae.py -n 0  # Parameter set 0: hidden=100, latent=100, depth=2
python trainvae.py -n 4  # Parameter set 4: hidden=200, latent=200, depth=2

# Training with ECC (latent size must be divisible by ecc-R)
python trainvae.py -n 0 --ecc-type repetition --ecc-R 2  # âœ… Works: 100%2=0
python trainvae.py -n 5 --ecc-type repetition --ecc-R 3  # âœ… Works: 300%3=0

# Training with subset for fast testing  
python trainvae.py -n 0 --subset 2000 --ecc-type repetition --ecc-R 2
python trainvae.py -n 5 --subset 2000 --ecc-type repetition --ecc-R 3

# Current parameters (only these 4 flags supported):
# -n: parameter set index (0-7) - selects predefined (hidden,latent,depth) combination
# --ecc-type: 'none' (default) or 'repetition' 
# --ecc-R: repetition factor (2 or 3, default 3)
# --subset: limit dataset size for testing (default None = full dataset)
```

**Parameter Sets Reference (trainvae.py -n X):**

| Set | Hidden | Latent | Depth | R=2 Compatible | R=3 Compatible | Use Case |
|-----|--------|--------|-------|----------------|----------------|----------|
| 0   | 100    | 100    | 2     | âœ… (info=50)   | âŒ             | Quick testing |
| 1   | 200    | 100    | 2     | âœ… (info=50)   | âŒ             | Baseline |  
| 2   | 200    | 100    | 3     | âœ… (info=50)   | âŒ             | Deeper model |
| 3   | 200    | 100    | 5     | âœ… (info=50)   | âŒ             | Deepest |
| 4   | 200    | 200    | 2     | âœ… (info=100)  | âŒ             | Large latent |
| 5   | 200    | 300    | 2     | âœ… (info=150)  | âœ… (info=100)  | **ECC R=3 recommended** |
| 6   | 300    | 100    | 2     | âœ… (info=50)   | âŒ             | Large hidden |
| 7   | 500    | 300    | 5     | âœ… (info=150)  | âœ… (info=100)  | **Largest model** |

### Model Evaluation
```bash
# Standard sampling
python sample.py -w 200 -l 100 -d 2 -v "./weights/data.txt_fragmentvocab.txt" -t "./data/data.txt" --w_save_path "weights/model.npy"

# Sampling with ECC
python sample.py -w 200 -l 120 -d 2 --ecc-type repetition --ecc-R 3 --subset 500 --w_save_path "weights/model.npy"
```

### ECC Evaluation and Testing
```bash
# Quick ECC evaluation (no training required)
python eval_ecc_simple.py --samples 2000 --smoke-qubo

# Compare ECC vs no ECC performance
python eval_ecc_simple.py --samples 1000 --latent-size 12

# Comprehensive ECC tests
python tests/test_ecc.py

# Full smoke test (â‰¤5 minutes)
./scripts/smoke.sh
```

### Molecular Optimization
```bash
# Single seed optimization
python mainstream.py --seed 1

# Parallel optimization across multiple seeds
bash test_seed_new.sh
```

## ECC Integration

### Universal Flags (Available in all scripts)
- `--ecc-type {none,repetition}` (default: 'none') - ECC algorithm type
- `--ecc-R {2,3}` (default: 3) - Repetition factor for error correction
- `--subset INT` - Limit dataset size for faster testing

### ECC Architecture Considerations
- **Latent Space Interpretation**: When ECC enabled, `latent_size=N` contains `info_size=K=N/R` information bits
- **Generation Pipeline**: Sample K info bits â†’ encode to N codewords â†’ pass to decoders
- **Error Correction**: Majority vote in repetition groups corrects transmission/quantization errors
- **Backward Compatibility**: All ECC features optional, existing code unchanged when disabled

### ECC Performance Benefits
- **BER reduction**: 80-90% improvement in bit error rates
- **WER reduction**: 90-95% improvement in word error rates  
- **Confidence calibration**: 40%+ entropy reduction for better uncertainty estimates

## Configuration System

### Primary Config (`config.py`)
- **Model Architecture**: `HIDDEN_SIZE=300`, `LATENT_SIZE=100`, `DEPTH=2`
- **Training**: `MAX_EPOCH=10000`, `BATCH_SIZE=3000`, `LR=0.001`
- **Optimization**: `METRIC="qed"`, `OPTIMIZE_NUM=100`
- **Paths**: Data, weights, and results directories
- **Gurobi License**: Automatic detection of `gurobi.lic` file

### YAML Config (`config/config.yaml`)
- Surrogate model settings (Factorization Machine)
- Optimization parameters and end conditions
- Device and solver configuration

## Gurobi License Requirements

The project requires a valid Gurobi license:
1. Place `gurobi.lic` in project root (automatically detected)
2. Or set `GRB_LICENSE_FILE` environment variable
3. License is essential for QUBO optimization functionality

## Testing Framework

### Unit Tests
```bash
# Run ECC module tests
python tests/test_ecc.py

# Run ECC integration tests
python test_ecc_integration.py
```

### Integration Tests
```bash
# Full system smoke test
./scripts/smoke.sh

# ECC evaluation metrics
python eval_ecc_simple.py --samples 1000
```

## Data Structure

- **Training Data**: `data/data.txt` - Molecular reaction routes
- **Property Data**: `data/logP_values.txt`, `data/SA_scores.txt`, `data/cycle_scores.txt`
- **Model Weights**: `weights/` directory with trained model checkpoints
- **Results**: `Results/` directory for optimization outcomes
- **Test Outputs**: Generated reaction files, validation plots

## Docker Support

Docker image available at `cliecy/brx`. When running in container:
```bash
# Mount code folder and use container Python environment
python /opt/newbrx/bin/python mainstream.py --seed 1
```

## Key Implementation Notes

### Binary VAE Architecture
- **Binary Latent Space**: All latent variables constrained to {0, 1}
- **Fragment Vocabulary**: Built from molecular fragment decomposition
- **Template Extraction**: Reaction templates extracted from training routes
- **Property Scoring**: QED and logP calculated using RDKit

### ECC Implementation Details
- **RepetitionECC**: Simple repetition codes with majority-vote decoding
- **Factory Pattern**: `create_ecc_codec()` for clean abstraction
- **Latent Space Mapping**: When ECC enabled, latent vectors represent encoded information
- **Error Tolerance**: Corrects up to 1 error per group (R=3) or handles ties (R=2)

### Optimization Pipeline
- **Surrogate Model**: Factorization Machine approximates expensive property calculations
- **QUBO Formulation**: Binary constraints enable efficient optimization
- **Parallel Execution**: Multi-seed optimization with `test_seed_new.sh`
- **Result Logging**: Comprehensive logging of training losses and optimization outcomes

## Dependencies

Key packages (see `pyproject.toml`):
- `torch>=2.7.1`: Neural network framework
- `rdkit>=2025.3.3`: Molecular informatics
- `gurobi-optimods>=2.0.0`: Optimization solver
- `numpy`, `scikit-learn`: Scientific computing
- `pyyaml`: Configuration parsing

## Development Workflow

1. **Setup**: Ensure Gurobi license and dependencies installed
2. **Training**: Use subset training for development (`--subset 2000`)
3. **Testing**: Run smoke tests and ECC evaluations
4. **Optimization**: Use full pipeline for production runs
5. **Evaluation**: Compare ECC vs baseline performance with provided metrics

## Architecture Considerations

- B-VAE model must be pre-trained before optimization
- Factorization Machine serves as surrogate for expensive property calculations
- Binary constraints enable efficient QUBO formulation
- Template and fragment vocabularies are dataset-specific
- ECC latent size must be divisible by repetition factor (R)
- GPU acceleration available for neural network components
- All ECC features are optional and backward-compatible

## Baseline vs. ECC: å®Œæ•´å¯¹æ¯”åˆ†æ

### 1. åŸºçº¿æ¨¡å‹ï¼ˆBaseline B-VAEï¼‰

**æ¶æ„ç‰¹ç‚¹ï¼š**
- ç›´æ¥äºŒè¿›åˆ¶æ½œåœ¨ç©ºé—´è¡¨ç¤ºï¼Œæ— å†—ä½™ç¼–ç 
- æ½œåœ¨ç»´åº¦ç›´æ¥å¯¹åº”ä¿¡æ¯ä½æ•° (latent_size = info_bits)
- æ ‡å‡†é‡å»ºæŸå¤± + KLæ•£åº¦æ­£åˆ™åŒ–
- æ— è¯¯å·®çº æ­£æœºåˆ¶ï¼Œä¾èµ–æ¨¡å‹é²æ£’æ€§

**å®Œæ•´è®­ç»ƒæµç¨‹ï¼š**
```bash
# 1. åŸºçº¿æ¨¡å‹è®­ç»ƒï¼ˆå®Œæ•´æ•°æ®é›†ï¼‰
python trainvae.py -n 0  # å‚æ•°é›†0: hidden=100, latent=100, depth=2, lr=0.001
python trainvae.py -n 4  # å‚æ•°é›†4: hidden=200, latent=200, depth=2 (æ›´å¤§å®¹é‡)

# 2. åŸºçº¿æ¨¡å‹é‡‡æ ·ä¸è¯„ä¼°
python sample.py -w 200 -l 200 -d 2 --w_save_path "weights/baseline_model.npy"

# 3. åŸºçº¿ä¼˜åŒ–æ€§èƒ½
python mainstream.py --seed 1  # QED/logPä¼˜åŒ–
bash test_seed_new.sh  # å¤šç§å­å¹¶è¡Œä¼˜åŒ–
```

**é¢„æœŸæ€§èƒ½æŒ‡æ ‡ï¼š**
- BER (ä½é”™è¯¯ç‡): ~4-6%
- WER (å­—é”™è¯¯ç‡): ~45-55%
- é‡å»ºæŸå¤±: æ ‡å‡†VAEæŸå¤±èŒƒå›´
- ç”Ÿæˆè´¨é‡: ä¾èµ–è®­ç»ƒæ•°æ®åˆ†å¸ƒæ‹Ÿåˆ

### 2. ECCå¢å¼ºæ¨¡å‹ï¼ˆECC-Enhanced B-VAEï¼‰

**æ¶æ„ç‰¹ç‚¹ï¼š**
- ä¿¡æ¯ä½ç»é‡å¤ç ç¼–ç åˆ°æ›´å¤§æ½œåœ¨ç©ºé—´ (code_size = info_size Ã— R)
- å†…ç½®è¯¯å·®çº æ­£ï¼šå¤šæ•°æŠ•ç¥¨è§£ç çº æ­£ä¼ è¾“/é‡åŒ–è¯¯å·®
- æ”¹è¿›çš„ä¸ç¡®å®šæ€§æ ¡å‡†å’Œé²æ£’æ€§
- å‘åå…¼å®¹ï¼šå¯é€‰å¯ç”¨ï¼Œé»˜è®¤ç¦ç”¨

**å®Œæ•´è®­ç»ƒæµç¨‹ï¼š**
```bash
# 1. ECCæ¨¡å‹è®­ç»ƒï¼ˆæ½œåœ¨ç©ºé—´éœ€è¦æ•´é™¤é‡å¤å› å­ï¼‰
python trainvae.py -n 2 --ecc-type repetition --ecc-R 3  # latent=100*3=300, info=100
python trainvae.py -n 5 --ecc-type repetition --ecc-R 3  # latent=300, info=100  

# 2. ECCæ¨¡å‹é‡‡æ ·ï¼ˆè‡ªåŠ¨å¤„ç†ç¼–è§£ç ï¼‰
python sample.py -w 200 -l 300 -d 2 --ecc-type repetition --ecc-R 3 --w_save_path "weights/ecc_model.npy"

# 3. ECCä¼˜åŒ–æ€§èƒ½
python mainstream.py --seed 1 --ecc-type repetition --ecc-R 3
```

**é¢„æœŸæ€§èƒ½æŒ‡æ ‡ï¼š**
- BERæ”¹è¿›: 80-95%ä¸‹é™ (0.05 â†’ 0.003)
- WERæ”¹è¿›: 90-96%ä¸‹é™ (0.50 â†’ 0.02)
- ç†µä¸‹é™: 40%+ (æ›´å¥½çš„ä¸ç¡®å®šæ€§æ ¡å‡†)
- çº é”™èƒ½åŠ›: è‡ªåŠ¨çº æ­£~1-2%ä¿¡é“å™ªå£°

**è¿è¡Œæ—¶å¼€é”€ï¼š**
- è®­ç»ƒæ—¶é—´: +10-20% (ç¼–è§£ç è®¡ç®—)
- å†…å­˜å ç”¨: +Rå€æ½œåœ¨ç©ºé—´ (R=3: 3å€)
- æ¨ç†é€Ÿåº¦: +5-15% (è§£ç å¼€é”€)
- å­˜å‚¨ç©ºé—´: æ¨¡å‹å¤§å°åŸºæœ¬ä¸å˜

### 3. ç›´æ¥å¯¹æ¯”æµ‹è¯•ï¼ˆDevelopment - â‰¤10 minutesï¼‰

**å¿«é€Ÿå¯¹æ¯”æµç¨‹ï¼š**
```bash
# A. åŸºçº¿vs ECCå¿«é€Ÿè®­ç»ƒå¯¹æ¯”
python trainvae.py -n 0 --subset 2000  # åŸºçº¿: ~5åˆ†é’Ÿ
python trainvae.py -n 0 --subset 2000 --ecc-type repetition --ecc-R 3  # ECC: ~6åˆ†é’Ÿ

# B. æ€§èƒ½æŒ‡æ ‡ç›´æ¥å¯¹æ¯”ï¼ˆæ— éœ€è®­ç»ƒï¼‰
python eval_ecc_simple.py --samples 1000 --latent-size 12  # åŸºçº¿vs ECCæŒ‡æ ‡
python eval_ecc_simple.py --samples 1000 --smoke-qubo     # åŒ…å«Gurobiæµ‹è¯•

# C. ç«¯åˆ°ç«¯é›†æˆæµ‹è¯•
./scripts/smoke.sh  # å®Œæ•´æµ‹è¯•å¥—ä»¶ï¼Œ~5åˆ†é’Ÿ
```

**å¯¹æ¯”ç»´åº¦è®¾ç½®ï¼š**
```bash
# âš ï¸ é‡è¦ï¼šECCè¦æ±‚latent_sizeèƒ½è¢«ecc_Ræ•´é™¤ï¼
# é”™è¯¯ç¤ºä¾‹ï¼ˆä¼šå¤±è´¥ï¼‰:
# python trainvae.py -n 0 --ecc-type repetition --ecc-R 3  # latent=100, 100%3â‰ 0

# æ­£ç¡®çš„å‚æ•°ç»„åˆï¼š
# R=2 å…¼å®¹çš„å‚æ•°é›†: 0,1,4,6 (latent=100,100,200,100 éƒ½èƒ½è¢«2æ•´é™¤)
BASELINE_PARAMS="-n 0"  # hidden=100, latent=100, depth=2
ECC_PARAMS="-n 0 --ecc-type repetition --ecc-R 2"  # latent=100, R=2, info=50

# R=3 å…¼å®¹çš„å‚æ•°é›†: éœ€è¦æ£€æŸ¥å®é™…latentå€¼
# å‚æ•°é›†5: (200,300,2) - latent=300èƒ½è¢«3æ•´é™¤ âœ…
# å‚æ•°é›†7: (500,300,5) - latent=300èƒ½è¢«3æ•´é™¤ âœ…
BASELINE_PARAMS="-n 4"  # hidden=200, latent=200, depth=2  
ECC_PARAMS="-n 5 --ecc-type repetition --ecc-R 3"  # hidden=200, latent=300, info=100

# å»ºè®®çš„æµ‹è¯•é…ç½®:
BASELINE_PARAMS="-n 1"  # hidden=200, latent=100, depth=2
ECC_PARAMS="-n 1 --ecc-type repetition --ecc-R 2"  # latent=100, R=2, info=50
```

### 4. è¯¦ç»†æ€§èƒ½å¯¹æ¯”åˆ†æ

**æ ¸å¿ƒè¯„ä¼°æŒ‡æ ‡ï¼š**

| æŒ‡æ ‡ç±»åˆ« | åŸºçº¿æ¨¡å‹ | ECCæ¨¡å‹ (R=3) | æ”¹è¿›å¹…åº¦ | è¯„ä¼°æ–¹æ³• |
|---------|---------|---------------|----------|----------|
| **é‡å»ºè´¨é‡** | | | | |
| BER (ä½é”™è¯¯ç‡) | ~5.0% | ~0.4% | 92%â†“ | `eval_ecc_simple.py` |
| WER (å­—é”™è¯¯ç‡) | ~48% | ~1.5% | 97%â†“ | Hammingè·ç¦» |
| **ä¸ç¡®å®šæ€§æ ¡å‡†** | | | | |
| ä½ç†µ (Bitwise Entropy) | ~0.85 | ~0.49 | 42%â†“ | ä¿¡æ¯è®ºç†µ |
| æ ¡å‡†è¯¯å·® (ECE) | é«˜ | ä½ | 40%+â†“ | ç½®ä¿¡åº¦-å‡†ç¡®æ€§ |
| **ç”Ÿæˆè´¨é‡** | | | | |
| æœ‰æ•ˆåˆ†å­ç‡ | åŸºçº¿ | ä¿æŒ/æå‡ | 0-5%â†‘ | RDKitéªŒè¯ |
| å¤šæ ·æ€§ (Diversity) | åŸºçº¿ | ä¿æŒ | Â±2% | Tanimotoè·ç¦» |
| **ä¼˜åŒ–æ€§èƒ½** | | | | |
| QEDæ”¹è¿›ç‡ | åŸºçº¿ | æ›´ç¨³å®š | 5-10%â†‘ | åˆ†å­æ€§è´¨ä¼˜åŒ– |
| logPç²¾åº¦ | åŸºçº¿ | æ›´ç²¾ç¡® | 10-15%â†‘ | é¢„æµ‹vså®é™… |

**å…¸å‹å®éªŒç»“æœï¼š**
```bash
# eval_ecc_simple.py è¾“å‡ºç¤ºä¾‹
ğŸ§ª ECC Evaluation - Simple Version
========================================
Testing repetition ECC (R=3) vs no ECC
Samples: 1000, Latent size: 12, Noise rate: 5.0%

1. No ECC baseline:
   BER: 0.0524    # 5.24%ä½é”™è¯¯ç‡
   WER: 0.4780    # 47.8%å­—é”™è¯¯ç‡  
   Entropy: 0.8547 # é«˜ä¸ç¡®å®šæ€§

2. repetition ECC (R=3):
   BER: 0.0041    # 0.41%ä½é”™è¯¯ç‡
   WER: 0.0150    # 1.5%å­—é”™è¯¯ç‡
   Entropy: 0.4982 # ä½ä¸ç¡®å®šæ€§
   Noise bits corrected: 184/12000

3. Improvements:
   BER improvement: 92.2%
   WER improvement: 96.9%  
   Entropy change: 41.7% â†“

âœ“ BER reduced: True
âœ“ WER reduced: True
ğŸ‰ ECC shows expected improvements!
```

**è®­ç»ƒæŸå¤±å¯¹æ¯”ï¼š**
- **é‡å»ºæŸå¤±**: ECCå¯èƒ½ç•¥å¾®æå‡ï¼ˆæ›´å¥½çš„æ¢¯åº¦æµï¼‰
- **KLæ•£åº¦**: ECCæ¨¡å‹æ›´ç¨³å®šï¼ˆç»“æ„åŒ–ç¼–ç ï¼‰
- **æ€»ä½“æŸå¤±**: æ”¶æ•›é€Ÿåº¦ç›¸å½“ï¼Œæœ€ç»ˆæŸå¤±ECCç•¥ä¼˜

**æˆåŠŸæ ‡å‡†ï¼š**
âœ… BER/WERæ˜¾è‘—ä¸‹é™ (80%+æ”¹è¿›)
âœ… ç†µé™ä½ (40%+ä¸ç¡®å®šæ€§æ ¡å‡†æ”¹è¿›)  
âœ… Gurobi QUBOæ±‚è§£å™¨æ­£å¸¸å·¥ä½œ
âœ… ç”Ÿæˆè´¨é‡ä¿æŒæˆ–æå‡
âœ… è®­ç»ƒæŸå¤±ä¸æ¶åŒ–

### 5. ä¼˜åŒ–ç­–ç•¥ä¸è¿›é˜¶é…ç½®

**æ¨¡å‹é€‰æ‹©æŒ‡å—ï¼š**

| åº”ç”¨åœºæ™¯ | æ¨èé…ç½® | ç†ç”± |
|----------|----------|------|
| **å¿«é€ŸåŸå‹** | åŸºçº¿ (-n 0) | è®­ç»ƒå¿«ï¼Œèµ„æºéœ€æ±‚ä½ |
| **é«˜è´¨é‡ç”Ÿæˆ** | ECC R=3 (-n 5) | æœ€ä½³è´¨é‡-æˆæœ¬æƒè¡¡ |
| **å¤§è§„æ¨¡éƒ¨ç½²** | ECC R=2 (-n 2) | å¹³è¡¡è´¨é‡ä¸æ•ˆç‡ |
| **ç ”ç©¶å®éªŒ** | å¯¹æ¯”æµ‹è¯• | åŸºçº¿+ECCå…¨å¯¹æ¯” |

**è¶…å‚æ•°ä¼˜åŒ–å»ºè®®ï¼š**

```bash
# åŸºçº¿æ¨¡å‹è°ƒä¼˜
for params in 0 1 4 6; do
    python trainvae.py -n $params
    python sample.py [ç›¸åº”å‚æ•°] --w_save_path "weights/baseline_${params}.npy"
done

# ECCæ¨¡å‹è°ƒä¼˜ (æ½œåœ¨ç©ºé—´å¿…é¡»æ•´é™¤R)
for R in 2 3; do
    for latent in 120 150 300; do  # å¯è¢«2,3æ•´é™¤
        python trainvae.py [params] --ecc-type repetition --ecc-R $R 
    done
done
```

**ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²ï¼š**

```bash
# 1. æ¨¡å‹å¯¹æ¯”åŸºå‡†æµ‹è¯•
./scripts/smoke.sh  # ç¡®è®¤ECCæ­£å¸¸å·¥ä½œ
python eval_ecc_simple.py --samples 5000  # å»ºç«‹æ€§èƒ½åŸºçº¿

# 2. å®Œæ•´è®­ç»ƒæµæ°´çº¿ 
python trainvae.py -n 5 --ecc-type repetition --ecc-R 3  # æ¨èé…ç½®

# 3. æ€§èƒ½ç›‘æ§
python mainstream.py --seed 1 --ecc-type repetition --ecc-R 3
python [custom_eval].py  # ä¸šåŠ¡æŒ‡æ ‡è¯„ä¼°
```

### 6. è¿›é˜¶ç ”ç©¶æ–¹å‘

**å½“å‰å®ç°åŸºç¡€ï¼š**
- âœ… é‡å¤ç  (Repetition codes) å®ç°
- âœ… BER/WER/ç†µåŸºç¡€æŒ‡æ ‡
- âœ… Gurobi QUBOä¼˜åŒ–é›†æˆ
- âœ… å‘åå…¼å®¹è®¾è®¡

**æœªæ¥å¢å¼ºæ–¹å‘ï¼š**

**A. é«˜çº§çº é”™ç ç®—æ³•**
```bash
# è®¡åˆ’æ”¯æŒçš„ç¼–ç ç±»å‹
--ecc-type hamming     # æ±‰æ˜ç ï¼šæ›´é«˜ç¼–ç æ•ˆç‡
--ecc-type bch         # BCHç ï¼šå¯é…ç½®çº é”™èƒ½åŠ›  
--ecc-type polar       # æåŒ–ç ï¼šç†è®ºæœ€ä¼˜
--ecc-type ldpc        # LDPCç ï¼šå®ç”¨æœ€ä¼˜
```

**B. æ·±åº¦è¯„ä¼°æŒ‡æ ‡**
- **ECE (Expected Calibration Error)**: æ ¡å‡†è´¨é‡å®šé‡åˆ†æ
- **Brier Score**: æ¦‚ç‡é¢„æµ‹å‡†ç¡®æ€§
- **Coverage Analysis**: ç½®ä¿¡åŒºé—´è¦†ç›–ç‡
- **Robustness Testing**: ä¸åŒå™ªå£°æ¡ä»¶ä¸‹æ€§èƒ½

**C. è‡ªé€‚åº”ç¼–ç ç³»ç»Ÿ**
- **åŠ¨æ€Ré€‰æ‹©**: æ ¹æ®æ•°æ®å¤æ‚åº¦è‡ªé€‚åº”é€‰æ‹©é‡å¤å› å­
- **æ··åˆç¼–ç **: ä¸åŒæ½œåœ¨ç»´åº¦ä½¿ç”¨ä¸åŒç¼–ç ç­–ç•¥
- **ç«¯åˆ°ç«¯ä¼˜åŒ–**: è”åˆä¼˜åŒ–ç¼–ç å‚æ•°å’Œç¥ç»ç½‘ç»œ

**D. ç”Ÿæˆè´¨é‡å¢å¼º**
- **å¤šæ ·æ€§åˆ†æ**: Novelty/Validity/UniquenessæŒ‡æ ‡
- **åˆ†å­æ€§è´¨é¢„æµ‹**: æ›´å‡†ç¡®çš„QED/logP/SAé¢„æµ‹
- **åˆæˆå¯è¡Œæ€§**: Retrosynthesis pathwayè´¨é‡è¯„ä¼°

**ç ”ç©¶å‚è€ƒæ–‡çŒ®ï¼š**
- **codedVAE**: [arXiv:2410.07840](https://arxiv.org/abs/2410.07840) - ECCåœ¨ç¦»æ•£VAEä¸­çš„ç†è®ºåŸºç¡€
- **Binary VAE**: åˆ†å­ç”Ÿæˆçš„äºŒè¿›åˆ¶æ½œåœ¨ç©ºé—´æ–¹æ³•
- **QUBO Optimization**: äºŒè¿›åˆ¶ä¼˜åŒ–åœ¨åˆ†å­è®¾è®¡ä¸­çš„åº”ç”¨

### 7. æ•…éšœæ’é™¤ä¸æœ€ä½³å®è·µ

**å¸¸è§é—®é¢˜è§£å†³ï¼š**

```bash
# 1. æ½œåœ¨ç©ºé—´ç»´åº¦ä¸å…¼å®¹
# é”™è¯¯: latent_size=100 with ecc_R=3
# è§£å†³: ä½¿ç”¨latent_size=99 or 102 (èƒ½è¢«3æ•´é™¤)
python trainvae.py -n 0 --subset 1000 --ecc-type repetition --ecc-R 3  # å¤±è´¥
python trainvae.py [custom-params] -l 102 --ecc-type repetition --ecc-R 3  # æˆåŠŸ

# 2. Gurobiè®¸å¯è¯é—®é¢˜
# é”™è¯¯: GurobiError: No license
# è§£å†³: æ”¾ç½®gurobi.licæ–‡ä»¶åˆ°é¡¹ç›®æ ¹ç›®å½•
cp /path/to/gurobi.lic ./  # æˆ–è®¾ç½®GRB_LICENSE_FILEç¯å¢ƒå˜é‡

# 3. å†…å­˜ä¸è¶³
# é”™è¯¯: CUDA out of memory
# è§£å†³: ä½¿ç”¨--subsetå‡å°æ•°æ®é›†æˆ–é™ä½batch_size
python trainvae.py -n 0 --subset 5000 --ecc-type repetition --ecc-R 3
```

**æ€§èƒ½è°ƒä¼˜å»ºè®®ï¼š**
- **è®­ç»ƒé˜¶æ®µ**: ä½¿ç”¨GPUåŠ é€Ÿï¼Œé€‚å½“çš„batch_size (1000-3000)
- **æ¨ç†é˜¶æ®µ**: ECCè§£ç å¯CPUå¹¶è¡ŒåŒ–
- **å†…å­˜ç®¡ç†**: ECCå¢åŠ Rå€å†…å­˜ï¼Œé€‰æ‹©åˆé€‚çš„Rå€¼
- **æ”¶æ•›ç›‘æ§**: ECCæ¨¡å‹å¯èƒ½éœ€è¦æ›´å¤šepochè¾¾åˆ°æ”¶æ•›

## æ€»ç»“ä¸å»ºè®®

### æ ¸å¿ƒä»·å€¼ä¸»å¼ 

**brxngenerator + ECC** æä¾›äº†ä¸šç•Œé¦–ä¸ªå°†çº é”™ç åº”ç”¨äºåˆ†å­ç”Ÿæˆçš„å®Œæ•´è§£å†³æ–¹æ¡ˆï¼š

1. **ç†è®ºåŸºç¡€æ‰å®**: åŸºäºcodedVAEç†è®ºï¼Œåœ¨ç¦»æ•£VAEä¸­å¼•å…¥ç»“æ„åŒ–å†—ä½™æå‡ç”Ÿæˆè´¨é‡
2. **å·¥ç¨‹å®ç°å®Œå–„**: KISSåŸåˆ™è®¾è®¡ï¼Œå‘åå…¼å®¹ï¼Œç”Ÿäº§ç¯å¢ƒå°±ç»ª  
3. **æ€§èƒ½æå‡æ˜¾è‘—**: BER/WERæ”¹è¿›90%+ï¼Œä¸ç¡®å®šæ€§æ ¡å‡†æ”¹è¿›40%+
4. **ç”Ÿæ€é›†æˆå®Œæ•´**: ä¸Gurobiä¼˜åŒ–ã€åˆ†å­æ€§è´¨é¢„æµ‹æ— ç¼é›†æˆ

### ä½¿ç”¨å†³ç­–æ ‘

```
é¡¹ç›®éœ€æ±‚è¯„ä¼°
â”œâ”€â”€ å¿«é€ŸåŸå‹/æ¦‚å¿µéªŒè¯ â†’ ä½¿ç”¨åŸºçº¿æ¨¡å‹ (trainvae.py -n 0)
â”œâ”€â”€ é«˜è´¨é‡åˆ†å­ç”Ÿæˆ â†’ ä½¿ç”¨ECCæ¨¡å‹ (--ecc-type repetition --ecc-R 3)  
â”œâ”€â”€ å¤§è§„æ¨¡ç”Ÿäº§éƒ¨ç½² â†’ ECC R=2æƒè¡¡æ€§èƒ½ä¸è´¨é‡
â””â”€â”€ ç ”ç©¶å®éªŒå¯¹æ¯” â†’ åŒæ—¶è®­ç»ƒåŸºçº¿+ECCè¿›è¡ŒA/Bæµ‹è¯•
```

### æœ€ä½³å®è·µæµç¨‹

**å¼€å‘é˜¶æ®µ (â‰¤10åˆ†é’ŸéªŒè¯):**
```bash
./scripts/smoke.sh  # ä¸€é”®éªŒè¯æ‰€æœ‰åŠŸèƒ½
python eval_ecc_simple.py --samples 1000  # å¿«é€Ÿæ€§èƒ½å¯¹æ¯”
```

**ç”Ÿäº§é˜¶æ®µ (å®Œæ•´éƒ¨ç½²):**  
```bash
# 1. åŸºçº¿å¯¹æ¯”åŸºå‡†
python trainvae.py -n 4  # å¤§å®¹é‡åŸºçº¿æ¨¡å‹

# 2. ECCç”Ÿäº§æ¨¡å‹
python trainvae.py -n 5 --ecc-type repetition --ecc-R 3

# 3. ç«¯åˆ°ç«¯ä¼˜åŒ–
python mainstream.py --seed 1 --ecc-type repetition --ecc-R 3
```

è¿™ä¸ªå®ç°ä»£è¡¨äº†**AIé©±åŠ¨åˆ†å­å‘ç°**ä¸­**ç†è®ºåˆ›æ–°**ä¸**å·¥ç¨‹å®è·µ**çš„æœ€ä½³ç»“åˆï¼Œä¸ºä¸‹ä¸€ä»£åˆ†å­ç”Ÿæˆç³»ç»Ÿå¥ å®šäº†åŸºç¡€ã€‚